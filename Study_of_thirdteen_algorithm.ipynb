{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes (GNB)\n",
    "\n",
    "  Bayes' theorem:条件付き確率に関して成り立つ定理\n",
    "  \n",
    "                  P(B|A)=P(A|B)P(B)/P(A)->P(B|A1,A2...,An)=P(B)P(A1,...,An|B)/P(A1,...,An)\n",
    "                  \n",
    "  Gaussian Naive Bayes->\n",
    "  \n",
    "    P(xi|y)=1*exp(-(xi-miu(y))^2/2sigma(y)^2)/root(2pi*sigma(y)^2)（正規分布の連続密度分布）\n",
    "  \n",
    "  Used in classification\n",
    "\n",
    "Bernoulli Naive Bayes (BNB)\n",
    "  \n",
    "  There may be multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable.If handed any other kind of data, a BernoulliNB instance may binarize its input.\n",
    "  \n",
    "  BenoulliNB might perform better better on some datasets, especially those with shorter documents.\n",
    "\n",
    "Multinomial Naive Bayes (MNB)\n",
    "  \n",
    "  Used in text classification\n",
    "  \n",
    "  尤度：ある前提条件に従って結果が出現する場合に、逆に観察結果からみて前提条件が「何々であった」と推測する尤もらしさ（もっともらしさ）を表す数値を、「何々」を変数とする関数として捉えたものである。\n",
    "  \n",
    "  最尤法->確率分布fDと分布の母数sitaののわかっている離散確率分布Dが与えられたとして、そこからN個の標本X1,X2...Xnを取り出すことを考えよう。すると分布関数から、観察されたデータが得られる確率を次のようにに計算することができる。\n",
    "  \n",
    "  例：箱の中から適当に1つ選んだコインを80回投げ、x1=H,x2=T,...x80=Tのようにサンプリングし、表(H)の観察された回数を数えたところ、表(H)が49回、裏(T)が31回であった,箱に入っているコインの数は無限であると仮定する。それぞれがすべての可能な0<=p<=1の値をとるとする。するとすべての可能な0<=p<=1の値に対して次の尤度関数を最大化しなければならない\n",
    "  \n",
    "  L(p)=fd(observe49 Heads out of 80|p)=C(80 49)p^49(1-p)^31\n",
    "  \n",
    "  微分\n",
    "  \n",
    "  0=d(C(80 49)p^49(1-p)^31)/dp->49p^48(1-p)^31-31p^49(1-p)^30\n",
    "   =p^48(1-p)^30[49(1-p)-31p]\n",
    "  \n",
    "  尤度を最大化するのは明らかにp=49/80\n",
    "\n",
    "Logistic Regression (LR)\n",
    "  \n",
    "  linear model for classification rather than regression\n",
    "  \n",
    "  This implementation can fit binary, One-vs- Rest, or multinomial logistic regression with optional L2 or L1 regularization.\n",
    "  \n",
    "  logit(pi)=ln(pi/1-pi)=alph+beta1x1,i+...+betakxk,i, i=1...n\n",
    "  \n",
    "  pi=E(Y|Xi)=Pr(Yi=1)\n",
    "  \n",
    "  pi=Pr(Yi=1|X)=1/1+e^(-(alph+beta1x1,i+...+betakxk,i))\n",
    "\n",
    "Stochastic Gradient Descent (SGD)\n",
    "  \n",
    "  Stochastic gradient descent is a simple yet very efficient approach to fit linear models. SGD has been successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing.\n",
    "  \n",
    "  mean_squared_loss:Using the MSE loss makes sense if the assumption that your outputs are a real-valued function of your inputs, with a certain amount of irreducible Gaussian noise, with constant mean and variance. If these assumptions don’t hold true (such as in the context of classification), the MSE loss may not be the best bet.\n",
    "  \n",
    "  huber:二乗誤差損失よりも外れ値に敏感ではない\n",
    "  \n",
    "  cross-entropy:binary classification, and the cross-entropy loss provides us with faster learning when our predictions differ significantly from our labels, as is generally the case during the first several iterations of model training.\n",
    "\n",
    "Passive Aggressive Classifier (PAC)\n",
    "  \n",
    "  重みの更新式：\n",
    "  f(x, w') = w' + sign(wx - y) * max(0, |wx - y| - ε) * x / (|x|^2 - 1/2C)\n",
    "  yは事前に学習データと一緒に用意してあげる人手で与えた正解スコア。εとCはユーザが自由に決められる値(パラメータ)。PA-IIは学習データに対する正解スコアとPA-IIが返すスコアの差がε以下になるように学習してくれる。Cはどれくらい分類の失敗を許すかというパラメータで大きいほど間違いにおおらかになる。\n",
    "  The passive-aggressive algorithms are a family of algorithms for large-scale learning. They are similar to the Perceptron in that they do not require a learning rate. However, contrary to the Perceptron, they include a regularization parameter C.For classification, PassiveAggressiveClassifier can be used with loss='hinge' (PA-I) or loss='squared_hinge' (PA-II). For regression, PassiveAggressiveRegressor can be used with loss='epsilon_insensitive' (PA-I) or loss='squared_epsilon_insensitive' (PA-II).\n",
    "\n",
    "Support Vector Classifier (SVC)\n",
    "  \n",
    "  基本概念為，在三次元(二次元)的空間上求距離最短(最佳)平面(分類)，藉此投影在平面上(線上)，距離作為weight，作為學習器。\n",
    "  \n",
    "  \n",
    "  Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
    "   \n",
    "  The advantages of support vector machines are:\n",
    "\n",
    "    Effective in high dimensional spaces.\n",
    "  \n",
    "    Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "  \n",
    "    Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "\n",
    "    Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "  The disadvantages of support vector machines include:\n",
    "\n",
    "    If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "    \n",
    "    SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).\n",
    "  \n",
    "  Tips on Practical Use:\n",
    "    \n",
    "    Avoiding data copy: For SVC, SVR, NuSVC and NuSVR, if the data passed to certain methods is not C-ordered contiguous, and double precision, it will be copied before calling the underlying C implementation. You can check whether a given numpy array is C-contiguous by inspecting its flags attribute.For LinearSVC (and LogisticRegression) any input passed as a numpy array will be copied and converted to the liblinear internal sparse data representation (double precision floats and int32 indices of non-zero components). If you want to fit a large-scale linear classifier without copying a dense numpy C-contiguous double precision array as input we suggest to use the SGDClassifier class instead. The objective function can be configured to be almost the same as the LinearSVC model.\n",
    "\n",
    "    Kernel cache size: For SVC, SVR, nuSVC and NuSVR, the size of the kernel cache has a strong impact on run times for larger problems. If you have enough RAM available, it is recommended to set cache_size to a higher value than the default of 200(MB), such as 500(MB) or 1000(MB).\n",
    "\n",
    "    Setting C: C is 1 by default and it’s a reasonable default choice. If you have a lot of noisy observations you should decrease it. It corresponds to regularize more the estimation.\n",
    "\n",
    "    Support Vector Machine algorithms are not scale invariant, so it is highly recommended to scale your data.For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. Note that the same scaling must be applied to the test vector to obtain meaningful results. See section Preprocessing data for more details on scaling and normalization.\n",
    "\n",
    "    Parameter nu in NuSVC/OneClassSVM/NuSVR approximates the fraction of training errors and support vectors.\n",
    "\n",
    "    In SVC, if data for classification are unbalanced (e.g. many positive and few negative), set class_weight='balanced' and/or try different penalty parameters C.\n",
    "\n",
    "    The underlying LinearSVC implementation uses a random number generator to select features when fitting the model. It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a smaller tol parameter.\n",
    "\n",
    "    Using L1 penalization as provided by LinearSVC(loss='l2', penalty='l1', dual=False) yields a sparse solution, i.e. only a subset of feature weights is different from zero and contribute to the decision function. Increasing C yields a more complex model (more feature are selected). The C value that yields a “null” model (all weights equal to zero) can be calculated using l1_min_c.\n",
    "  \n",
    "\n",
    "K-Nearest Neighbor (KNN)\n",
    "\n",
    "  越近的點擁有更大的投票權。\n",
    "    \n",
    "  あるオブジェクトの分類は、その近傍のオブジェクト群の投票によって決定される（すなわち、k 個の最近傍のオブジェクト群で最も一般的なクラスをそのオブジェクトに割り当てる）。k は正の整数で、一般に小さい。k = 1 なら、最近傍のオブジェクトと同じクラスに分類されるだけである。二項分類の場合、k を奇数にすると同票数で分類できなくなる問題を避けることができる。\n",
    "  同じ手法は回帰分析に使われる。この場合、オブジェクトの属性値を k 個の最近傍のオブジェクト群の属性値の平均値とする。より近いオブジェクトに大きく重み付けすることもできる。\n",
    "  \n",
    "  The basic nearest neighbors regression uses uniform weights: that is, each point in the local neighborhood contributes uniformly to the classification of a query point. Under some circumstances, it can be advantageous to weight points such that nearby points contribute more to the regression than faraway points. This can be accomplished through the weights keyword. The default value, weights = 'uniform', assigns equal weights to all points. weights = 'distance' assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied, which will be used to compute the weights.\n",
    "  \n",
    "  Nearest neighbors has been successful in a large number of classification and regression problems, including handwritten digits or satellite image scenes.\n",
    "  \n",
    "\n",
    "Decision Tree (DT)\n",
    "\n",
    "  利用簡單的決定樹，從資料中去制定規則。深度越深，決定越複雜。決定樹對於大量的資料很容易造成overfitting，必須要調整最大深度與子樹數。\n",
    "\n",
    "  Decision trees tend to overfit on data with a large number of features. Getting the right ratio of samples to number of features is important, since a tree with few samples in high dimensional space is very likely to overfit.\n",
    "  Consider performing dimensionality reduction (PCA, ICA, or Feature selection) beforehand to give your tree a   better chance of finding features that are discriminative.\n",
    "  Visualise your tree as you are training by using the export function. Use max_depth=3 as an initial tree depth to get a feel for how the tree is fitting to your data, and then increase the depth.\n",
    "  Remember that the number of samples required to populate the tree doubles for each additional level the tree grows to. Use max_depth to control the size of the tree to prevent overfitting.\n",
    "  Use min_samples_split or min_samples_leaf to control the number of samples at a leaf node. A very small number will usually mean the tree will overfit, whereas a large number will prevent the tree from learning the data. Try min_samples_leaf=5 as an initial value. If the sample size varies greatly, a float number can be used as percentage in these two parameters. The main difference between the two is that min_samples_leaf guarantees a minimum number of samples in a leaf, while min_samples_split can create arbitrary small leaves, though min_samples_split is more common in the literature.\n",
    "  Balance your dataset before training to prevent the tree from being biased toward the classes that are dominant. Class balancing can be done by sampling an equal number of samples from each class, or preferably by normalizing the sum of the sample weights (sample_weight) for each class to the same value. Also note that weight-based pre-pruning criteria, such as min_weight_fraction_leaf, will then be less biased toward dominant classes than criteria that are not aware of the sample weights, like min_samples_leaf.\n",
    "  If the samples are weighted, it will be easier to optimize the tree structure using weight-based pre-pruning criterion such as min_weight_fraction_leaf, which ensure that leaf nodes contain at least a fraction of the overall sum of the sample weights.\n",
    "  All decision trees use np.float32 arrays internally. If training data is not in this format, a copy of the dataset will be made.\n",
    "  If the input matrix X is very sparse, it is recommended to convert to sparse csc_matrix before calling fit and sparse csr_matrix before calling predict. Training time can be orders of magnitude faster for a sparse matrix input compared to a dense matrix when features have zero values in most of the samples.\n",
    "\n",
    "Random Forest (RF):averaging methods\n",
    "\n",
    "  Empirical good default values are max_features=n_features for regression problems, and max_features=sqrt(n_features) for classification tasks (where n_features is the number of features in the data). Good results are often achieved when setting max_depth=None in combination with min_samples_split=2\n",
    "\n",
    "Extra Trees Classifier (ERF)\n",
    "\n",
    "AdaBoost (AB):boosting methods\n",
    "\n",
    "  adaboost的基本概念為，利用連續的弱學習器(只稍微比猜得好)，重複地去修改資料的版本\n",
    "    \n",
    "  The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights w_1, w_2, …, w_N to each of the training samples. Initially, those weights are all set to w_i = 1/N, so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly.\n",
    "\n",
    "Gradient Tree Boosting (GTB):boosting methods\n",
    "\n",
    "  The advantages of GBRT are:\n",
    "\n",
    "    Natural handling of data of mixed type (= heterogeneous features)\n",
    "    \n",
    "    Predictive power\n",
    "    \n",
    "    Robustness to outliers in output space (via robust loss functions)\n",
    "  \n",
    "  The disadvantages of GBRT are:\n",
    "    \n",
    "    Scalability, due to the sequential nature of boosting it can hardly be parallelized."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
