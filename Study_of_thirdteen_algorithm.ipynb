{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes (GNB)\n",
    "\n",
    "  Bayes' theorem:条件付き確率に関して成り立つ定理\n",
    "  \n",
    "                  P(B|A)=P(A|B)P(B)/P(A)->P(B|A1,A2...,An)=P(B)P(A1,...,An|B)/P(A1,...,An)\n",
    "                  \n",
    "  Gaussian Naive Bayes->\n",
    "  \n",
    "    P(xi|y)=1*exp(-(xi-miu(y))^2/2sigma(y)^2)/root(2pi*sigma(y)^2)（正規分布の連続密度分布）\n",
    "  \n",
    "  Used in classification\n",
    "\n",
    "Bernoulli Naive Bayes (BNB)\n",
    "  \n",
    "  There may be multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable.If handed any other kind of data, a BernoulliNB instance may binarize its input.\n",
    "  \n",
    "  BenoulliNB might perform better better on some datasets, especially those with shorter documents.\n",
    "\n",
    "Multinomial Naive Bayes (MNB)\n",
    "  \n",
    "  Used in text classification\n",
    "  \n",
    "  尤度：ある前提条件に従って結果が出現する場合に、逆に観察結果からみて前提条件が「何々であった」と推測する尤もらしさ（もっともらしさ）を表す数値を、「何々」を変数とする関数として捉えたものである。\n",
    "  \n",
    "  最尤法->確率分布fDと分布の母数sitaののわかっている離散確率分布Dが与えられたとして、そこからN個の標本X1,X2...Xnを取り出すことを考えよう。すると分布関数から、観察されたデータが得られる確率を次のようにに計算することができる。\n",
    "  \n",
    "  例：箱の中から適当に1つ選んだコインを80回投げ、x1=H,x2=T,...x80=Tのようにサンプリングし、表(H)の観察された回数を数えたところ、表(H)が49回、裏(T)が31回であった,箱に入っているコインの数は無限であると仮定する。それぞれがすべての可能な0<=p<=1の値をとるとする。するとすべての可能な0<=p<=1の値に対して次の尤度関数を最大化しなければならない\n",
    "  \n",
    "  L(p)=fd(observe49 Heads out of 80|p)=C(80 49)p^49(1-p)^31\n",
    "  \n",
    "  微分\n",
    "  \n",
    "  0=d(C(80 49)p^49(1-p)^31)/dp->49p^48(1-p)^31-31p^49(1-p)^30\n",
    "   =p^48(1-p)^30[49(1-p)-31p]\n",
    "  \n",
    "  尤度を最大化するのは明らかにp=49/80\n",
    "\n",
    "Logistic Regression (LR)\n",
    "  \n",
    "  linear modell for classification rather than regression\n",
    "  \n",
    "  This implementation can fit binary, One-vs- Rest, or multinomial logistic regression with optional L2 or L1 regularization.\n",
    "  \n",
    "  logit(pi)=ln(pi/1-pi)=alph+beta1x1,i+...+betakxk,i, i=1...n\n",
    "  \n",
    "  pi=E(Y|Xi)=Pr(Yi=1)\n",
    "  \n",
    "  pi=Pr(Yi=1|X)=1/1+e^(-(alph+beta1x1,i+...+betakxk,i))\n",
    "\n",
    "Stochastic Gradient Descent (SGD)\n",
    "  \n",
    "  Stochastic gradient descent is a simple yet very efficient approach to fit linear models.\n",
    "  \n",
    "  mean_squared_loss:Using the MSE loss makes sense if the assumption that your outputs are a real-valued function of your inputs, with a certain amount of irreducible Gaussian noise, with constant mean and variance. If these assumptions don’t hold true (such as in the context of classification), the MSE loss may not be the best bet.\n",
    "  \n",
    "  huber:二乗誤差損失よりも外れ値に敏感ではない\n",
    "  \n",
    "  cross-entropy:binary classification, and the cross-entropy loss provides us with faster learning when our predictions differ significantly from our labels, as is generally the case during the first several iterations of model training.\n",
    "\n",
    "Passive Aggressive Classifier (PAC)\n",
    "  \n",
    "  The passive-aggressive algorithms are a family of algorithms for large-scale learning. They are similar to the Perceptron in that they do not require a learning rate. However, contrary to the Perceptron, they include a regularization parameter C.For classification, PassiveAggressiveClassifier can be used with loss='hinge' (PA-I) or loss='squared_hinge' (PA-II). For regression, PassiveAggressiveRegressor can be used with loss='epsilon_insensitive' (PA-I) or loss='squared_epsilon_insensitive' (PA-II).\n",
    "\n",
    "Support Vector Classifier (SVC)\n",
    "  \n",
    "  Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
    "   \n",
    "  The advantages of support vector machines are:\n",
    "\n",
    "  Effective in high dimensional spaces.\n",
    "  \n",
    "  Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "  \n",
    "  Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "\n",
    "  Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    "  The disadvantages of support vector machines include:\n",
    "\n",
    "  If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "  \n",
    "  SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).\n",
    "  \n",
    "  \n",
    "\n",
    "K-Nearest Neighbor (KNN)\n",
    "\n",
    "Decision Tree (DT)\n",
    "\n",
    "Random Forest (RF)\n",
    "\n",
    "Extra Trees Classifier (ERF)\n",
    "\n",
    "AdaBoost (AB)\n",
    "\n",
    "Gradient Tree Boosting (GTB)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
