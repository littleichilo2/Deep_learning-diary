#Scaling Series Data in Python 
There are two types of scaling of your series that you may want to consider: normalization and standardization.
1.Normalize Series Data
  Normalization is a rescaling of the data from the original range so that all values are within the range of 0 and 1.
  Normalization requires that you know or are able to accurately estimate the minimum and maximum observable values. If your time series is trending up or down, estimating these expected values may be difficult and normalization may not be the best method to use on your problem.
2.Standardize Series Data
  Standardizing a dataset involves rescaling the distribution of values so that the mean of observerd values is 0 and the standard deviation is 1.
  This can be thought of as subtracting the meanb value or centering the data.
  Like normalization, standardization can be useful, and even required in some machine learning algorithms when your data has input values with differing scales.
  Standardization assumes that your observations fit a Gaussian distribution(bell curve) with a well behaved mean and standard deviation.You can still atandardize your time series data if this expectation is not met, but may not get reliable results.
#Practical Considerations When Scaling
1.Estinate Coefficients.
  You can estimate coefficients (min and max values for normalization or mean and standard deviation for standardization) from the training data.Inspect these firs-cut estimates and use domain knowkedge or domain experts to help improve these estimates so that they will be usefully correct on all data in the future.
2.Save Coefficients
  You will need to normalize new data in the future in exactly the way as the data used to train your model. Save the coefficients used to file and load them later when you need to scale new data when making predictions.
3.Data Analysis
  Use data analysis to help you better understand your data. For example, a simple histogram can help you quickly get a feeling for the distribution of quandtities to see if standardization would make sense. 
4.Scale Each Series.
  If your problem has multiple series, treat each as a separate variable and in turn scale them separately.
5.Scale At The Right Time.
  It is important to apply any scaling transforms at the right time. For example, if you have a series of quantities that is non-stationary. It would not be appropriate to scale the series after it has been transformed into a supervised learning problem as each column would be handled differently, which would be incorrect.
6.Scale if in Doubt.
  You probably do need to rescale your input and output variables. If in doubt, at least normalize your data.
  
#Scaling Input Variables
  The input varibles are those that the network takes on the input or visible layer in order to make a prediction.
  Whether input variables require scaling depends on the specifics of your problen and of each variable.
1.Categorical Inputs
  You may have a sequence of categorical inputs, such as letters  or statuses.
  Generally, categorical inputs are first integer encoded then one hot encoded. That is, a unique integer value is assigned to each distinct possibleinput, then a binary vector of ones and zeros is used to represent each integer value.
  By definition, a one hot encoding will ensure that each input is a small real value, in this case 0.0 or 1.0.
2.Real-Valued Inputs
  You may have a sequence of quantities as inputs, such as prices or temperatures.
  If the distribution of the quantity is normal, then it should be standardized, otherwise the series should be normalized. This applies if the range of quantity values is large(10s 100s,etc.) or small (0.01,0.0001).
  If the quantity values are small (near 0-1) and the distribution is limited (e.g. standard deviation near 1) then perhaps you can get away with no scaling of the series.

reference from:
https://machinelearningmastery.com/how-to-scale-data-for-long-short-term-memory-networks-in-python/
