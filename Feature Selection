What is Feature Selection
  Feature selection is also called variable selection or attribute selection.
  It is the auyo matic selection of attributes in your data(such as columns in tabular data) that are most relevent to the predictive modeling problem you are working on.
    
    Feature selection is the process of selecting a subset of relevant features for use in model construction.
  
  Feature selection is different from dimensionality reduction. Both methods seek to reduce the number of attributes in the dataset, but a dimensionality reduction method do so by creating new combinations of attributes, where as feature selection methods include and exclude attributes present in the data without changing them.
  Examples of dimensionality reduction methods include Principal Component Analysis , Singlar Value Decomposition and Sammon's Mapping.
The Problem The Feature Selection Solves
  Feature selection methods aid you in your mission to create an accurate predictive model. They help you by choosing features that will give you as good or better accuracy whilst requiring less data.
  Feature selection methods can be used to identify and remove unneeded, irrelevant and redundant attributes from data that do not contribute to the accuracy of a predictive model or may in fact decrease the accuracy of the model.
  Fewer attributes is desirable because it reduces the complexity of the model, and a simpler model is simpler to understand and explain.
Frature Selection Algorithms
  There are three general classes of feature selection algorithms: filter methods, wrapper methods and embedded methods.
Filter Methods 
  Filter feature selection methods apply a statistical measure to assign a scoring to each feature. The features are ranked by the score and either selected to be kept or removed from the dataset. The methods are often univariate and consider the feature independently, or with regard to the dependent variable.
  Some examples of some filter methods include the Chi squared test, information gain and coefficient scores.
Wrapper Methods
  Wrapper methods consider the selection of a set of features as a search problem, where different combinations are prepared, evaluted and compared to other combinations. A predictive model us used to evaluate a combinations of features and assign a score base on model accuracy.
  The search process may be methodical such as a best-first search, it may stochastic such as a random hill-climbing algorithm, or it may use heuristics, like forward and backward passes to add and remove features.
  An example if a wrapper method is the recursive feature elimination algorithm.
Embedded Methods
  Embedded methods learn which features best contribute to the accuracy of the model while the model is being created. The most common type of embedded feature selection methods are regularization methods.
  Regularization methods are also called penalization methods that introduce additional constraints into the optimization of a predictive algorithm (such as a regression algorithm) that bias the model toward lower complexity (fewer coefficients).
  Examples of regularization algorithms are the LASSO, Elastic Net and Ridge Regression.
Feature Selection Tutorials and Recipes
  We have seen a number of examples of features selection before on this blog.
    *Weka: For a tutorial showing how to perform feature selection using Weka see "https://machinelearningmastery.com/feature-selection-to-improve-accuracy-and-decrease-training-time/"
    *Scikit-Learn: For a recipe of Recurisive Feature Elimination in Python using scikit-learn, see "https://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/"
    *R:For a recipe of Recursive Feature Elimination using the Caret R package, see "https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/"
  A Trap When Selection Features
    Feature selection is another key part of the applied machine learning process, like model selection. You cannot fire and forget.
    It is important to consider feature selection a part of the model selection proceds. If you do not, you may inadvertently introduce bias into your models which can result in overfitting.
      
      Should do feature selection on a different dataset than you train[your predictive model] on ...the effect of not doing this is you will overfit your training data.
    
    For example, you must include feature selection within the inner-loop when you are using accuracy estimation methods suchas cross-validtion. This means that feature selection is performed on the prepared fold right before the model is trained. A mistake would be to perform feature selection and training on the selected features.
      
      If we adopt the proper procedure, and perform feature selection in each fold, there is no longer any information about the held out cases in the choice of features used in that fold.
      
    The reason is that the decisions made to select the features were made on the entire training set, that in turn
