Tips to Improve Deep Learning Performance
1.Improve Performance With Data
  1)Get More Data.
    The quality of your models is generally constrained by the quality of your training data. You want the best data you can get for your problem.
    You also want lots of it.
    Deep learning and other modern nonlinear machine learning techniques get better with more data. Deep learning especially. It is one of the main points that make deep learning so exciting.
    More data does not always help, but it can. If I am given the choice, I will get more data for the optionality it provides.
  2)Invent More Data.
    If you can't reasonably get more data, you can invent more data.
    *If your data are images, create randomly modified versions of existing images.
    *If your data are vectors of numbers, create randomly modified versions of existing vectors.
    *If your data are text, you get the idea...
  3)Rescale Your Data
    This is a quick win.
    A traditional rule of thumb when working with neural networks is:
    Rescale your data to the bounds of your activation functions.
    If you are using sigmoid activation functions, rescale your data to values between 0-and-1. If you’re using the Hyperbolic Tangent (tanh), rescale to values between -1 and 1.
    This applies to inputs (x) and outputs (y). For example, if you have a sigmoid on the output layer to predict binary values, normalize your y values to be binary. If you are using softmax, you can still get benefit from normalizing your y values.
    This is still a good rule of thumb, but I would go further.
    I would suggest that you create a few different versions of your training dataset as follows:
    *Normalized to 0 to 1.
    *Rescaled to -1 to 1.
    *Standardized.
    Then evaluate the performance of your model on each. Pick one, then double down.
    If you change your activation functions, repeat this little experiment.
    Big values accumulating in your network are not good. In addition, there are other methods for keeping numbers small in your network such as normalizing activation and weights, but we’ll look at these techniques later. 
  4)Transform Your data.
    Related to rescaling suggested above,but more work.
    You must really get to know your data. Visualize it. Look for outliers.
    Gusstimate the univariate distribution of each column.
    *Does a column look like a skewed Gaussian, consider adjusting the skew with a Box-Cox transform.
    *Does a column look like it has some features, but they are being clobbered by something obvious, try squaring, or square-rooting.
    *Does a column look like an exxponential distribution, consider a log transform.
    *Can you make a feature discrete or binned in some way to better emphasize.
    Lean on your intuition. Try things.
    *Can you pre-process data with a projection method PCA?
    *Can you aggregate multiple attributes into a single value?
    *Can you expose some interesting aspect of the problem with a new boolean flag?
    *Can you explore temporal or other structure in some other way?
    Neural nets perform feature learning. They can do this stuff.
    But they will also learn a problem much faster if you can better expose the structure of the problem to the network for learning.
    Spot-check lots of different transforms of your data or of specific attributes and see what works and what doesn’t.
  5)Feature Selection
    Neural nets are generally robust to unrelated data.
    They’ll use a near-zero weight and sideline the contribution of non-predictive attributes.
    Still, that’s data, weights, training cycles used on data not needed to make good predictions.
    Can you remove some attributes from your data?
    There are lots of feature selection methods and feature importance methods that can give you ideas of features to keep and features to boot.
    Try some. Try them all. The idea is to get ideas.
    Again, if you have time, I would suggest evaluating a few different selected “Views” of your problem with the same network and see how they perform.
    *Maybe you can do as well or better with fewer features. Yay, faster!
    *Maybe all the feature selection methods boot the same specific subset of features. Yay, consensus on useless features.
    *Maybe a selected subset gives you some ideas on further feature engineering you can perform. Yay, more ideas. 
  6)Reframe Your Problem.
    Step back from your problem.
    Are the observations that you’ve collected the only way to frame your problem?
    Maybe there are other ways. Maybe other framings of the problem are able to better expose the structure of your problem to learning.
    I really like this exercise because it forces you to open your mind. It’s hard. Especially if you’re invested (ego!!!, time, money) in the current approach.
    Even if you just list off 3-to-5 alternate framings and discount them, at least you are building your confidence in the chosen approach.
    *Maybe you can incorporate temporal elements in a window or in a method that permits timesteps.
    *Maybe your classification problem can become a regression problem, or the reverse.
    *Maybe your binary output can become a softmax output?
    *Maybe you can model a sub-problem instead.
    It is a good idea to think through the problem and it’s possible framings before you pick up the tool, because you’re less invested in solutions.
    Nevertheless, if you’re stuck, this one simple exercise can deliver a spring of ideas.
    Also, you don’t have to throw away any of your prior work. See the ensembles section later on. 
2.Improve Performance With Algorithms
  Machine learning is about algorithms.
  All the theory and math descrives differents approached to learn a decision process from data (if we constrain ourselves to predictive modeling).
  You’ve chosen deep learning for your problem. Is it really the best technique you could have chosen?
  In this section, we’ll touch on just a few ideas around algorithm selection before next diving into the specifics of getting the most from your chosen deep learning method.
  Here’s the short list
  1.Spot-Check Algorithms.
  2.Steal From Literature.
  3.Resampling Methods.
  Let’s get into it.
  1)Spot-Check Algorithms
    Brace yourself.
    You cannot know which algorithm will perform best on your problem beforehand.
    If you knew, you probably would not need machine learning.
    What evidence have you collected that your chosen method was a good choice?
    Let's flip this conunfrum.
    No single algorithm can perform better than any other, when performance is averaged across all possible problems. All algorithms are equal. This is a summary of the finding form the no free lunch theorem.

    Maybe your chosen algorithms is not the best for yout problem.

    Now, we are not trying to solve all possible problems, but the new hotness in algorithm land may not be the best choice on your specific dataset.
    My advice is to collect evidence. Entertain the idea that there are other food algorithms and given them a fair well and which do not.
    Spot-check a suite of top methods and see which fair well and which do not.
    *Evaluate some linear methods like logistic regression and linear discriminate analysis.
    *Evaluate some tree methods like CART, Random Forest and Gradient Boosting.
    *Evaluate some instance methods like SVM and kNN.
    *Evaluate some other neual network methods like LVQ,MLP,CNN,LSTM,hybrids,etc.
    Double down on the top performers and improve their chance with some further tuning or data preparation.
    Rank the results against your chosen deep learning method, how do they compare?
    Maybe you can drop the deep learning model and use something a lot simpler, a lot faster to trainm even something that is easy to understand.
  2)Steal From Literature
    A great shortcut to picking a good method, is to steal ideas from literature.
    Who else has worked on a problem like yours and what methods did they use.
    Check papers, books, blog posts,Q&A sites, tutorials, everything Google throws at you.
    Write down all the ideas and work your way through them.
    This is not about replicating research, it is about new ideas that you have not thought of that may give you a lift in performance.
    
    Published research is highly optimized.
    
    There are a lot of smart people writing lots of interesting things. Mine this great library for nuggets you need.
  3)Resamplng Methods
    You must know how good your models are.
    Is your estimate of the performance of your models reliable?
    Deep learning methods are slow to train.
    This ofter means we cannot use glod standard methods to estimate the performace od the model such as k-fold cross validation.
    *Maybe you are using a simple train/test split, this is very common. If so, you need to ensure that the split is representative of the problem. Univariate stats and visualization are a good start.
    *Maybe you can exploit hardware to improve the estimates. For example, if you have a cluster or an Amazon Web Services account, we can train n-models in parallel then take the mean and standard deviation of the results to get a more robust estimate.
    *Maybe you can use a validation hold out set to get an idea of the performance of the model as it trains (useful for early stopping, see later).
    *Maybe you can hold back a completely blind validation set that you use only after you have performed model selection.
    Going the other way, maybe you can make the dataset smaller and use stronger resampling methods.
    *Maybe you see a strong correlation with the performance of the model trained on a sample of the training dataset as to one trained on the whole dataset. Perhaps you can perform model selection and tuning using the smaller dataset, then scale the final technique up to the full dataset at the end.
    *Maybe you can constrain the dataset anyway, take a sample and use that for all model development.
    
    You must have complete confidence in the performance estimates of your models.
    
3.Improve Performance With Algorithm Tuning
  1)Diagnostics
    You will get better performance if you know why performance is no longer improving.
    Is your model overfitting or underfitting?
     
    Always keep this question in mind. Always.
    
    It will be doing one or the other, just by varying degrees.
    A quick way to get insight into the learning behavior of your model is to evaluate it on the training and a validation dataset each epoch, and plot the results.
    *If training is much better than the validation set, you are probably overfitting and you can use techniques like regularization.
    *If training and validation are both low, you are probably underfitting and you can probably increase the capacity of your network and train more or longer.
    *If there is an inflection point when training goes above the validation, you might be able to use eary stopping.
    Create these plots often and study them for insight into the different techniques you can use to improve performance.
    
    These plots might be the most valuable diagnostics you can create.
    
    Another useful diagnostic is to study the observations that the network gets right and wrong.
    On some problems, this can give you ideas of things to try.
    *Perhaps you need more or augmented examples of the difficult-to-train on examples.
    *Perhaps you can remove large samples of the training dataset that are wasy to model.
    *Perhaps you can use specialized models that focus on different clear regions of the input space.
  2)Weight Initialization
    The rule of thumb used to be:
    
    Initializa using small random numbers.
  
    In practice, that is still probably good enoughl. But is it the best for your network?
    There are also heuristics for different activation functions, but I don't remeber seeing much difference in practice.
    Keep your network fixed and try each initialization scheme.
    Remember, the weights are the actual parameters if your modelthat your are trying to find.There are many sets of weights that give good performance , but you want better performace
    *Try all the different initilization methods offered and see if one is better with all else held constant.
    *Try pre-learning with an unsupervised method like an autoencoder.
    *Try taking an existing and retraining a new input and output layer for your problem(transder learning)
    
    Remember, changing the weight initialization method is closely tied with the activation function and even the optimization function.
    
  3)Learning Rate
    There is often payoff in tuning the learning rate.
    Here are some ideas id things to explore:
    *Experiment with very large and very small learning rates.
    *Grid search common learning rate values from the literature and see how far you can push the network.
    *Try a learning rate that decreases over epochs.
    *Try a learning rate that drops every fixed number of epochs by a percentage.
    *Try adding a momentum term then grid search learning rate and momebtum together.
    Larger networks need more trainin, and the reverse. If you add more neurons or more layers, increase your learning rate.
    Learning rate is coupled with the number of training epochs, batch size and optimization method. 
  4)Activation Functions
    You probably should be using rectifier activation functions.
    They just work better.
    Before that it was sigmoid and tanh, then a softmax, linear or sigmoid on the output layer. I don't recommend trying more than that unless you know what you're doing.
    Try all three though and rescale your data to meet the bounds of the functions.
    Obviously, you want to choose the right transfer function for the form of your output, but consider exploring different representations.
    For example, switch your sigmoid for binary classification to linear for a regression problem, then post-process your outputs. This may also require changing the loss function to something more appropriate. See the section on Data Transforms for more ideas aling these lines.
  5)Network Topology
    Changes to your network structure will pay off.
    How many layers and how many neurons do you need?
    No one knows. No one. Don't ask.
    You must discover a good configuration for your problem. Experiment.
    *Try one hidden layer with a lot of neurons.
    *Try  a deep network with few neurons per layer(deep).
    *Try conbinations of the above.
    *Try architectures form recent papers on problems similar to yours.
    *Try topology patterns (fan out then in) and rules of thumb from books and papers.
    It's hard.
    Larger networks have a greater representational capability, and maybe you need it.
    More layers offer more opportunity for hierarchical re-composition of abstract features learned form the data. Maybe you need that.
    Later networks need more training, both in epochs and in learning rate.
    Adjust accordingly.
    
    
    
    
    
    
  
  
  
