https://qiita.com/icoxfog417/items/8689f943fd1225e24358

1.判断根拠を理解することの意義
  機械学習モデルの判断根拠を理解することで、こうした攻撃をうけた際に「普段とは違う根拠」であることに気付ける可能性があります。
  それに、別に攻撃受けなくても純粋データの傾向（ユーザーの行動など）が変わることで精度か落ちることも在ります。この場合でもモデル
  が依拠していた特徴量が分かれば対応がとりやすいです。つまり自分たちの利用しているモデルをメンテンナスしていくという面でも、判断
  根拠の解釈が重要になってくるということです。

2.判断根拠の「理解」の定義
  ここからは、実際にディープラーニングの判断根拠を「理解」する方法について見ていきます。
  まず、モデルの理解という面では以下二つの観点があります。
  ・仕組の理解：モデルの中で、どのような作用が働いて出力に至っているのか、というプロセスを理解する。ホワイトボックス的な理解。
  ・挙動の理解：どんな入力をしたらどんな出力が出るかだけ理解する。ブラックボックス的な理解（中で何がどうなっているのかは気にし
  ない）。
  ここでは「挙動の理解」に焦点を当てます。具体的には学習済みのモデルが与えられている状態からスタートし、どんな入力をしたらどん
  な出力が得られるのかを「説明」できれば「理解」ができたこととします。自動販売機ていえばどんなボタンを押したらどんなドリンクが
  出てくるか理解できればOKで、ボタンを押したら内部の機械のあれにコレな信号が走って．．．という内部の仕組みは置いておく、という
  ことです。
  さて、「説明」は人間に対して行うので、当然人間が理解可能な「表現」で行われる必要があります。ただ、「表現」できれば説明できた
  ことになるかというとそうではありません。先ほど示したネットワークの重みを可視化した画像のように、人間が認識可能な表現で会って
  もそれが説明の差異を、以下のように定義します。
  
  ・表現：人間が理解可能な表現、具体的には画像（可視化）やテキストの変換する。
  ・説明：ネットワークの出力に貢献を行っている特徴量（入力画像内のピクセルや、入力テキスト中の単語etc）を、「表現」すること
  
  つまり、「説明」とは単に可視化や文章化といった「表現」を行うことではなく、「ネットワークの出力に貢献を行っている特徴量」を「
  表現」して初めて「説明」になる、ということです。つまり、説明を行うためには出力に対する各入力の影響度を算出すること、それをひ
  とが理解可能な形て表現すること、の二つの工程が含まれます。以後紹介するにおいても、この2点について工夫がとられています。
  
3.学習されたネットワークの「説明」の挑戦する
  ここからは、実際にネットワークの出力を説明する手法を紹介していただきます。手法は、大きく以下のように分けられます。
  
  1.ネットワークの出力を最大化する入力を作成する。
  2.入力に対する感度を分析する。
  3.出力から入力までの経路を逆にたどる
  4.様々な入力から出力の傾向を推定する
  5.変更量から判断基準を類推する
  6.学習結果が予測可能になるよう矯正する
  7.入力に対する着眼点をモデルに組み込み
  
  最後の着眼点をモデルに組み込み（attention）についてはネットワーク自体に手を入れる必要があるためブラックボックスという観点から
  外れてしまいますが、非常に有効な手法であるためここで紹介をします。
  
4.説明力の評価
  上記では様々な「説明」を行う手法を紹介してきましたが、ではその「説明力」を検証する方法はあるでしょうか、その観点としては、以下
  の2点があります。
  ・説明の一貫性
      ある入力に対する説明は、その入力に近いデータに対する説明と近しいはず＝似ているデータは似ている説明がなされるはず。
  ・説明の正当性
      説明において重要とされている特徴量は、モデルの判断にとっても重要な特徴量のはず＝もし説明において重要とされている特徴量を抜
      いたら、モデルの判断に大きな影響を与えるはず。
  ・実装普遍性
      ネットワークの構成は違っても出力は全く同じ場合、説明の手法は2つのネットワーク間で一致するはず
